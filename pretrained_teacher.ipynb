{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import transformers, datasets, accelerate, tensorboard, evaluate\n",
    "from models import Student\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from util import *\n",
    "from loss import LossCalulcator\n",
    "from pretrained_kd import *\n",
    "from datasets import Array3D, ClassLabel, Features, load_dataset, Image\n",
    "from matplotlib import pyplot\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW, ViTFeatureExtractor, ViTModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "temperature = 10\n",
    "distillw = 0.1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "epochs = 100\n",
    "lr = 0.1\n",
    "lr_stepsize = 20\n",
    "batch_size = 1000\n",
    "test_batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"aaraki/vit-base-patch16-224-in21k-finetuned-cifar10\")\n",
    "teacher = AutoModelForImageClassification.from_pretrained(\"aaraki/vit-base-patch16-224-in21k-finetuned-cifar10\")\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student = Student(3, 32, 10, 0.2)\n",
    "student.load_state_dict(torch.load('model/cifar10_github/epoch_99.bin'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf692a328a74ac485a45a6ee1d2e997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/5.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d2f0321dd3f400cac41774c48bf569e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/120M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d34c9856e6141c3aaf51b059d8fb5bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/23.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8e8d745c384d608266d9748438df2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46ccb6c64724fdb99b8d53508130781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe0a2533be14ab9913c1bf28afa614f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2b3187640747918db54a127a317d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5fae3371fe480dbbe39116a1a38e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"cifar10\")\n",
    "split = dataset['train'].train_test_split(test_size=(5000.0/50000))\n",
    "dataset['splitted_train'] = split['train']\n",
    "dataset['validation'] = split['test']\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "def preprocess_images(examples):\n",
    "    # get batch of images\n",
    "    images = examples['img']\n",
    "    examples['img'] = [np.array(image) for image in examples['img']]\n",
    "    # convert to list of NumPy arrays of shape (C, H, W)\n",
    "    images = [np.array(image, dtype=np.uint8) for image in images]\n",
    "    images = [np.moveaxis(image, source=-1, destination=0) for image in images]\n",
    "    # preprocess and add pixel_values\n",
    "    inputs = feature_extractor(images=images)\n",
    "    examples['pixel_values'] = inputs['pixel_values']\n",
    "    return examples\n",
    "\n",
    "features = Features({\n",
    "    'label': ClassLabel(names=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']),\n",
    "    # 'img': Image(decode=True, id=None),\n",
    "    # could probably change img to int for faster inference\n",
    "    'img': Array3D(dtype=\"float32\", shape=(3,32,32)),\n",
    "    'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)), })\n",
    "\n",
    "# preprocessed_train = dataset['train'].map(preprocess_images, batched=True, features=features)\n",
    "preprocessed_val = dataset['validation'].map(preprocess_images, batched=True, features=features)\n",
    "preprocessed_test = dataset['test'].map(preprocess_images, batched=True, features=features)\n",
    "preprocessed_splitted_train = dataset['splitted_train'].map(preprocess_images, batched=True, features=features)\n",
    "\n",
    "# set format to PyTorch\n",
    "# preprocessed_train.set_format('torch', columns=['img', 'pixel_values', 'label'])\n",
    "preprocessed_val.set_format('torch', columns=['img', 'pixel_values', 'label'])\n",
    "preprocessed_test.set_format('torch', columns=['img', 'pixel_values', 'label'])\n",
    "preprocessed_splitted_train.set_format('torch', columns=['img', 'pixel_values', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataloaders = {}\n",
    "preprocessed_dataloaders['splitted_train'] = torch.utils.data.DataLoader(preprocessed_splitted_train, batch_size=batch_size, shuffle=True)\n",
    "preprocessed_dataloaders['validation'] = torch.utils.data.DataLoader(preprocessed_val, batch_size=batch_size)\n",
    "preprocessed_dataloaders['test'] = torch.utils.data.DataLoader(preprocessed_test, batch_size=test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_dataloaders = construct_dataloaders((preprocessed_train, preprocessed_test, preprocessed_splitted_train, preprocessed_val), batch_size, shuffle_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12718605995178223\n",
      "tensor([[ 3.3682, -0.3160, -0.2798, -0.5006, -0.5529, -0.5625, -0.6144, -0.4671,\n",
      "          0.2807, -0.3066]])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "teacher_input = processor(images=dataset['train']['img'][0], return_tensors=\"pt\")\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    teacher_output = teacher(**teacher_input)\n",
    "print(time.time()-start)\n",
    "print(teacher_output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6, 7, 0, 4, 9,\n",
      "        5, 2, 4, 0, 9, 6, 6, 5, 4, 5, 9, 2, 4, 1, 9, 5, 4, 6, 5, 6, 0, 9, 3, 9,\n",
      "        7, 6, 9, 8, 0, 3, 8, 8, 7, 7, 4, 6, 7, 3, 6, 3, 6, 2, 1, 2, 3, 7, 2, 6,\n",
      "        8, 8, 0, 2, 9, 3, 3, 8, 8, 1, 1, 7, 2, 5, 2, 7, 8, 9, 0, 3, 8, 6, 4, 6,\n",
      "        6, 0, 0, 7]), 'img': tensor([[[[158., 112.,  49.,  ...,  41., 161., 116.],\n",
      "          [ 41., 160., 111.,  ..., 109.,  44., 149.],\n",
      "          [107.,  45., 150.,  ..., 116.,  85.,  33.],\n",
      "          ...,\n",
      "          [133., 122., 133.,  ..., 132., 103.,  57.],\n",
      "          [183., 183., 175.,  ..., 226., 220., 191.],\n",
      "          [188., 164., 135.,  ..., 165., 154., 157.]],\n",
      "\n",
      "         [[147., 137., 155.,  ..., 104.,  81.,  46.],\n",
      "          [188., 191., 189.,  ..., 188., 199., 171.],\n",
      "          [164., 170., 142.,  ..., 201., 192., 160.],\n",
      "          ...,\n",
      "          [ 87.,  80.,  77.,  ...,  61.,  57.,  46.],\n",
      "          [ 52.,  54.,  38.,  ..., 120., 105.,  63.],\n",
      "          [117., 120., 124.,  ..., 225., 133., 140.]],\n",
      "\n",
      "         [[144., 134., 141.,  ...,  49.,  58.,  34.],\n",
      "          [ 54.,  68.,  49.,  ...,  55.,  90., 115.],\n",
      "          [ 79., 105., 133.,  ..., 252., 208., 224.],\n",
      "          ...,\n",
      "          [ 54., 107., 160.,  ..., 105.,  22.,  66.],\n",
      "          [101.,  29.,  73.,  ..., 111., 145.,  53.],\n",
      "          [ 96., 131.,  52.,  ...,  21.,  67., 110.]]],\n",
      "\n",
      "\n",
      "        [[[235., 235., 235.,  ..., 232., 233., 233.],\n",
      "          [233., 233., 233.,  ..., 232., 234., 232.],\n",
      "          [231., 234., 232.,  ..., 232., 232., 232.],\n",
      "          ...,\n",
      "          [233., 243., 201.,  ..., 238., 240., 239.],\n",
      "          [109., 130., 141.,  ..., 234., 231., 236.],\n",
      "          [234., 230., 237.,  ..., 206., 213., 164.]],\n",
      "\n",
      "         [[184., 191., 146.,  ..., 237., 240., 239.],\n",
      "          [195., 212., 224.,  ..., 222., 219., 226.],\n",
      "          [225., 210., 221.,  ..., 204., 208., 175.],\n",
      "          ...,\n",
      "          [165., 113., 131.,  ..., 109., 148.,  85.],\n",
      "          [101., 139.,  79.,  ..., 136., 149., 156.],\n",
      "          [ 13.,  25.,  41.,  ...,  70.,  42.,  51.]],\n",
      "\n",
      "         [[ 77.,  48.,  59.,  ...,  67., 106.,  53.],\n",
      "          [ 68., 104.,  53.,  ..., 137., 146., 149.],\n",
      "          [ 36.,  46.,  55.,  ...,  15.,   0.,   2.],\n",
      "          ...,\n",
      "          [ 85., 101.,  83.,  ...,  41.,  53.,  58.],\n",
      "          [ 44.,  55.,  62.,  ..., 127., 107., 118.],\n",
      "          [124., 106., 114.,  ..., 186., 200., 199.]]],\n",
      "\n",
      "\n",
      "        [[[158., 190., 222.,  ..., 245., 209., 222.],\n",
      "          [244., 206., 218.,  ..., 241., 249., 232.],\n",
      "          [239., 243., 234.,  ..., 238., 241., 246.],\n",
      "          ...,\n",
      "          [108., 131.,  86.,  ..., 123., 120., 117.],\n",
      "          [ 66.,  78.,  89.,  ..., 118., 102., 106.],\n",
      "          [113., 103., 111.,  ...,  96., 111.,  66.]],\n",
      "\n",
      "         [[ 87., 104.,  95.,  ...,  94., 104., 112.],\n",
      "          [ 53.,  74.,  88.,  ..., 146.,  95., 113.],\n",
      "          [127.,  93., 115.,  ..., 113., 123.,  85.],\n",
      "          ...,\n",
      "          [190., 144., 171.,  ..., 163., 173., 154.],\n",
      "          [163., 173., 152.,  ...,  65.,  72.,  78.],\n",
      "          [ 99., 132., 165.,  ..., 171., 125., 148.]],\n",
      "\n",
      "         [[171., 129., 154.,  ..., 153., 162., 130.],\n",
      "          [143., 152., 114.,  ...,  27.,  33.,  39.],\n",
      "          [ 62.,  94., 124.,  ..., 126.,  78.,  99.],\n",
      "          ...,\n",
      "          [ 28.,  41.,  47.,  ...,  55.,  36.,  49.],\n",
      "          [ 53.,  34.,  45.,  ...,  33.,  44.,  27.],\n",
      "          [ 34.,  45.,  27.,  ...,   7.,   8.,   7.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[132., 135., 149.,  ..., 151., 136., 139.],\n",
      "          [151., 134., 139.,  ..., 140., 152., 134.],\n",
      "          [139., 151., 134.,  ..., 130., 137., 152.],\n",
      "          ...,\n",
      "          [140., 154., 132.,  ..., 134., 139., 151.],\n",
      "          [140., 140., 152.,  ..., 153., 137., 141.],\n",
      "          [153., 136., 141.,  ..., 141., 154., 134.]],\n",
      "\n",
      "         [[140., 154., 134.,  ..., 135., 139., 151.],\n",
      "          [138., 139., 151.,  ..., 153., 135., 141.],\n",
      "          [153., 134., 140.,  ..., 141., 154., 137.],\n",
      "          ...,\n",
      "          [151., 138., 141.,  ..., 141., 151., 138.],\n",
      "          [141., 152., 138.,  ..., 137., 139., 150.],\n",
      "          [137., 139., 149.,  ..., 150., 139., 142.]],\n",
      "\n",
      "         [[151., 139., 142.,  ..., 141., 150., 139.],\n",
      "          [142., 151., 139.,  ..., 136., 138., 150.],\n",
      "          [139., 140., 149.,  ..., 149., 138., 141.],\n",
      "          ...,\n",
      "          [ 13.,  24.,   8.,  ...,  50., 112., 118.],\n",
      "          [121., 136., 139.,  ..., 137., 146., 132.],\n",
      "          [137., 146., 133.,  ..., 134., 136., 143.]]],\n",
      "\n",
      "\n",
      "        [[[255., 255., 255.,  ..., 189., 255., 255.],\n",
      "          [255., 255., 254.,  ..., 255., 255., 255.],\n",
      "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
      "          ...,\n",
      "          [ 93.,  83.,  39.,  ..., 255., 254., 255.],\n",
      "          [255., 255., 255.,  ..., 254., 254., 254.],\n",
      "          [254., 251., 252.,  ..., 105.,  65.,  44.]],\n",
      "\n",
      "         [[ 68.,  23.,  55.,  ..., 255., 255., 255.],\n",
      "          [255., 255., 255.,  ..., 255., 254., 254.],\n",
      "          [254., 255., 255.,  ...,  82.,  36.,  59.],\n",
      "          ...,\n",
      "          [ 62., 100., 132.,  ..., 255., 253., 242.],\n",
      "          [247., 241., 182.,  ..., 255., 255., 255.],\n",
      "          [255., 255., 255.,  ...,  66., 110., 142.]],\n",
      "\n",
      "         [[ 82., 121., 155.,  ..., 254., 254., 255.],\n",
      "          [255., 255., 236.,  ..., 255., 255., 255.],\n",
      "          [255., 255., 255.,  ...,  51., 139., 156.],\n",
      "          ...,\n",
      "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
      "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
      "          [255., 255., 255.,  ..., 255., 255., 255.]]],\n",
      "\n",
      "\n",
      "        [[[234., 233., 238.,  ..., 238., 235., 233.],\n",
      "          [238., 237., 235.,  ..., 240., 243., 242.],\n",
      "          [240., 243., 242.,  ..., 249., 248., 251.],\n",
      "          ...,\n",
      "          [244., 247., 245.,  ..., 253., 251., 252.],\n",
      "          [235., 224., 221.,  ...,  98., 155., 107.],\n",
      "          [ 93., 159., 105.,  ..., 200., 200., 237.]],\n",
      "\n",
      "         [[235., 239., 233.,  ..., 235., 232., 237.],\n",
      "          [188., 150., 139.,  ...,  66.,  76.,  64.],\n",
      "          [ 66., 103.,  79.,  ..., 156., 153., 232.],\n",
      "          ...,\n",
      "          [ 52.,  96.,  70.,  ...,  47.,  46.,  60.],\n",
      "          [ 55.,  53.,  84.,  ..., 197., 176., 131.],\n",
      "          [143.,  88.,  67.,  ...,  40., 126., 105.]],\n",
      "\n",
      "         [[ 87., 161., 141.,  ...,  66.,  66.,  72.],\n",
      "          [ 69.,  67.,  78.,  ..., 146., 146., 108.],\n",
      "          [146.,  90.,  69.,  ...,  45.,  82.,  70.],\n",
      "          ...,\n",
      "          [106.,  71.,  54.,  ...,  99., 165., 150.],\n",
      "          [108., 184., 163.,  ..., 177., 128., 198.],\n",
      "          [183., 134., 206.,  ..., 199., 178., 127.]]]]), 'pixel_values': tensor([[[[ 0.2392,  0.2392,  0.2392,  ..., -0.0902, -0.0902, -0.0902],\n",
      "          [ 0.2392,  0.2392,  0.2392,  ..., -0.0902, -0.0902, -0.0902],\n",
      "          [ 0.2392,  0.2392,  0.2392,  ..., -0.0902, -0.0902, -0.0902],\n",
      "          ...,\n",
      "          [-0.5765, -0.5765, -0.5765,  ..., -0.8353, -0.8353, -0.8353],\n",
      "          [-0.5765, -0.5765, -0.5765,  ..., -0.8353, -0.8353, -0.8353],\n",
      "          [-0.5765, -0.5765, -0.5765,  ..., -0.8353, -0.8353, -0.8353]],\n",
      "\n",
      "         [[-0.1216, -0.1216, -0.1216,  ..., -0.3333, -0.3333, -0.3333],\n",
      "          [-0.1216, -0.1216, -0.1216,  ..., -0.3333, -0.3333, -0.3333],\n",
      "          [-0.1216, -0.1216, -0.1216,  ..., -0.3333, -0.3333, -0.3333],\n",
      "          ...,\n",
      "          [-0.1608, -0.1608, -0.1608,  ..., -0.4745, -0.4745, -0.4745],\n",
      "          [-0.1608, -0.1608, -0.1608,  ..., -0.4745, -0.4745, -0.4745],\n",
      "          [-0.1608, -0.1608, -0.1608,  ..., -0.4745, -0.4745, -0.4745]],\n",
      "\n",
      "         [[-0.6157, -0.6157, -0.6157,  ..., -0.7412, -0.7412, -0.7412],\n",
      "          [-0.6157, -0.6157, -0.6157,  ..., -0.7412, -0.7412, -0.7412],\n",
      "          [-0.6157, -0.6157, -0.6157,  ..., -0.7412, -0.7412, -0.7412],\n",
      "          ...,\n",
      "          [ 0.2549,  0.2549,  0.2549,  ..., -0.1373, -0.1373, -0.1373],\n",
      "          [ 0.2549,  0.2549,  0.2549,  ..., -0.1373, -0.1373, -0.1373],\n",
      "          [ 0.2549,  0.2549,  0.2549,  ..., -0.1373, -0.1373, -0.1373]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8431,  0.8431,  0.8431,  ...,  0.8196,  0.8196,  0.8196],\n",
      "          [ 0.8431,  0.8431,  0.8431,  ...,  0.8196,  0.8196,  0.8196],\n",
      "          [ 0.8431,  0.8431,  0.8431,  ...,  0.8196,  0.8196,  0.8196],\n",
      "          ...,\n",
      "          [-0.3333, -0.3333, -0.3333,  ...,  0.4588,  0.4588,  0.4588],\n",
      "          [-0.3333, -0.3333, -0.3333,  ...,  0.4588,  0.4588,  0.4588],\n",
      "          [-0.3333, -0.3333, -0.3333,  ...,  0.4588,  0.4588,  0.4588]],\n",
      "\n",
      "         [[ 0.8431,  0.8431,  0.8431,  ...,  0.8196,  0.8196,  0.8196],\n",
      "          [ 0.8431,  0.8431,  0.8431,  ...,  0.8196,  0.8196,  0.8196],\n",
      "          [ 0.8431,  0.8431,  0.8431,  ...,  0.8196,  0.8196,  0.8196],\n",
      "          ...,\n",
      "          [-0.2078, -0.2078, -0.2078,  ...,  0.5686,  0.5686,  0.5686],\n",
      "          [-0.2078, -0.2078, -0.2078,  ...,  0.5686,  0.5686,  0.5686],\n",
      "          [-0.2078, -0.2078, -0.2078,  ...,  0.5686,  0.5686,  0.5686]],\n",
      "\n",
      "         [[ 0.8431,  0.8431,  0.8431,  ...,  0.8196,  0.8196,  0.8196],\n",
      "          [ 0.8431,  0.8431,  0.8431,  ...,  0.8196,  0.8196,  0.8196],\n",
      "          [ 0.8431,  0.8431,  0.8431,  ...,  0.8196,  0.8196,  0.8196],\n",
      "          ...,\n",
      "          [-0.3490, -0.3490, -0.3490,  ...,  0.5608,  0.5608,  0.5608],\n",
      "          [-0.3490, -0.3490, -0.3490,  ...,  0.5608,  0.5608,  0.5608],\n",
      "          [-0.3490, -0.3490, -0.3490,  ...,  0.5608,  0.5608,  0.5608]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2392,  0.2392,  0.2392,  ...,  0.8667,  0.8667,  0.8667],\n",
      "          [ 0.2392,  0.2392,  0.2392,  ...,  0.8667,  0.8667,  0.8667],\n",
      "          [ 0.2392,  0.2392,  0.2392,  ...,  0.8667,  0.8667,  0.8667],\n",
      "          ...,\n",
      "          [-0.7804, -0.7804, -0.7804,  ..., -0.9451, -0.9451, -0.9451],\n",
      "          [-0.7804, -0.7804, -0.7804,  ..., -0.9451, -0.9451, -0.9451],\n",
      "          [-0.7804, -0.7804, -0.7804,  ..., -0.9451, -0.9451, -0.9451]],\n",
      "\n",
      "         [[ 0.4902,  0.4902,  0.4902,  ...,  0.8902,  0.8902,  0.8902],\n",
      "          [ 0.4902,  0.4902,  0.4902,  ...,  0.8902,  0.8902,  0.8902],\n",
      "          [ 0.4902,  0.4902,  0.4902,  ...,  0.8902,  0.8902,  0.8902],\n",
      "          ...,\n",
      "          [-0.6784, -0.6784, -0.6784,  ..., -0.9373, -0.9373, -0.9373],\n",
      "          [-0.6784, -0.6784, -0.6784,  ..., -0.9373, -0.9373, -0.9373],\n",
      "          [-0.6784, -0.6784, -0.6784,  ..., -0.9373, -0.9373, -0.9373]],\n",
      "\n",
      "         [[ 0.7412,  0.7412,  0.7412,  ...,  0.9294,  0.9294,  0.9294],\n",
      "          [ 0.7412,  0.7412,  0.7412,  ...,  0.9294,  0.9294,  0.9294],\n",
      "          [ 0.7412,  0.7412,  0.7412,  ...,  0.9294,  0.9294,  0.9294],\n",
      "          ...,\n",
      "          [-0.6314, -0.6314, -0.6314,  ..., -0.9451, -0.9451, -0.9451],\n",
      "          [-0.6314, -0.6314, -0.6314,  ..., -0.9451, -0.9451, -0.9451],\n",
      "          [-0.6314, -0.6314, -0.6314,  ..., -0.9451, -0.9451, -0.9451]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0353,  0.0353,  0.0353,  ...,  0.0196,  0.0196,  0.0196],\n",
      "          [ 0.0353,  0.0353,  0.0353,  ...,  0.0196,  0.0196,  0.0196],\n",
      "          [ 0.0353,  0.0353,  0.0353,  ...,  0.0196,  0.0196,  0.0196],\n",
      "          ...,\n",
      "          [-0.8980, -0.8980, -0.8980,  ...,  0.0510,  0.0510,  0.0510],\n",
      "          [-0.8980, -0.8980, -0.8980,  ...,  0.0510,  0.0510,  0.0510],\n",
      "          [-0.8980, -0.8980, -0.8980,  ...,  0.0510,  0.0510,  0.0510]],\n",
      "\n",
      "         [[ 0.0588,  0.0588,  0.0588,  ...,  0.0745,  0.0745,  0.0745],\n",
      "          [ 0.0588,  0.0588,  0.0588,  ...,  0.0745,  0.0745,  0.0745],\n",
      "          [ 0.0588,  0.0588,  0.0588,  ...,  0.0745,  0.0745,  0.0745],\n",
      "          ...,\n",
      "          [-0.8118, -0.8118, -0.8118,  ...,  0.0667,  0.0667,  0.0667],\n",
      "          [-0.8118, -0.8118, -0.8118,  ...,  0.0667,  0.0667,  0.0667],\n",
      "          [-0.8118, -0.8118, -0.8118,  ...,  0.0667,  0.0667,  0.0667]],\n",
      "\n",
      "         [[ 0.1686,  0.1686,  0.1686,  ...,  0.1922,  0.1922,  0.1922],\n",
      "          [ 0.1686,  0.1686,  0.1686,  ...,  0.1922,  0.1922,  0.1922],\n",
      "          [ 0.1686,  0.1686,  0.1686,  ...,  0.1922,  0.1922,  0.1922],\n",
      "          ...,\n",
      "          [-0.9373, -0.9373, -0.9373,  ...,  0.1216,  0.1216,  0.1216],\n",
      "          [-0.9373, -0.9373, -0.9373,  ...,  0.1216,  0.1216,  0.1216],\n",
      "          [-0.9373, -0.9373, -0.9373,  ...,  0.1216,  0.1216,  0.1216]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          ...,\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8353,  0.8353,  0.8353,  ...,  0.9529,  0.9529,  0.9529],\n",
      "          [ 0.8353,  0.8353,  0.8353,  ...,  0.9529,  0.9529,  0.9529],\n",
      "          [ 0.8353,  0.8353,  0.8353,  ...,  0.9529,  0.9529,  0.9529],\n",
      "          ...,\n",
      "          [-0.1686, -0.1686, -0.1686,  ...,  0.5608,  0.5608,  0.5608],\n",
      "          [-0.1686, -0.1686, -0.1686,  ...,  0.5608,  0.5608,  0.5608],\n",
      "          [-0.1686, -0.1686, -0.1686,  ...,  0.5608,  0.5608,  0.5608]],\n",
      "\n",
      "         [[ 0.8275,  0.8275,  0.8275,  ...,  0.9451,  0.9451,  0.9451],\n",
      "          [ 0.8275,  0.8275,  0.8275,  ...,  0.9451,  0.9451,  0.9451],\n",
      "          [ 0.8275,  0.8275,  0.8275,  ...,  0.9451,  0.9451,  0.9451],\n",
      "          ...,\n",
      "          [-0.4431, -0.4431, -0.4431,  ...,  0.3961,  0.3961,  0.3961],\n",
      "          [-0.4431, -0.4431, -0.4431,  ...,  0.3961,  0.3961,  0.3961],\n",
      "          [-0.4431, -0.4431, -0.4431,  ...,  0.3961,  0.3961,  0.3961]],\n",
      "\n",
      "         [[ 0.8667,  0.8667,  0.8667,  ...,  0.9686,  0.9686,  0.9686],\n",
      "          [ 0.8667,  0.8667,  0.8667,  ...,  0.9686,  0.9686,  0.9686],\n",
      "          [ 0.8667,  0.8667,  0.8667,  ...,  0.9686,  0.9686,  0.9686],\n",
      "          ...,\n",
      "          [-0.5765, -0.5765, -0.5765,  ..., -0.0039, -0.0039, -0.0039],\n",
      "          [-0.5765, -0.5765, -0.5765,  ..., -0.0039, -0.0039, -0.0039],\n",
      "          [-0.5765, -0.5765, -0.5765,  ..., -0.0039, -0.0039, -0.0039]]]])}\n",
      "student:  0.07259511947631836\n",
      "tensor([[-4.3361e+02, -6.1733e+02, -1.1268e+03, -5.6714e+02, -6.4385e+02,\n",
      "         -8.9683e+02, -7.7042e+02, -8.2970e+02, -4.6920e+02,  1.6831e+01],\n",
      "        [-5.5508e+02, -9.8133e+02, -1.4018e+03, -7.3127e+02, -7.6216e+02,\n",
      "         -1.3361e+03, -1.1883e+03, -9.8263e+02, -6.4890e+02, -1.3096e+02],\n",
      "        [-7.1095e+02, -1.2942e+02, -1.2673e+03, -8.9502e+02, -7.7468e+02,\n",
      "         -6.7559e+02, -1.1847e+03, -3.0898e+02, -3.5447e+02,  8.2579e+01],\n",
      "        [-7.0122e+02, -2.6280e+02, -1.3272e+03, -7.0667e+02, -3.1864e+02,\n",
      "         -9.2253e+02, -1.1305e+03, -3.8219e+02, -5.7706e+02, -2.3821e+02],\n",
      "        [-5.6972e+02, -4.9682e+02, -9.6820e+02, -3.0809e+02, -1.7769e+02,\n",
      "         -7.8377e+02, -7.2826e+02, -5.1283e+02, -6.7589e+02, -3.8365e+02],\n",
      "        [-2.2415e+02, -8.0343e+02, -5.9881e+02, -3.9757e+02, -5.6852e+02,\n",
      "         -7.7590e+02, -5.9530e+02, -5.3943e+02, -4.5573e+02, -2.9650e+02],\n",
      "        [-5.6941e+02, -6.3037e+02, -9.2565e+02, -4.4317e+02, -4.7710e+02,\n",
      "         -8.2220e+02, -6.2393e+02, -6.3749e+02, -5.0853e+02, -1.9822e+02],\n",
      "        [-4.3545e+02, -6.3276e+02, -1.0206e+03, -3.8448e+02, -4.7249e+02,\n",
      "         -7.5304e+02, -7.2312e+02, -5.1914e+02, -5.2611e+02, -1.3269e+01],\n",
      "        [-1.1903e+03, -3.0806e+02, -1.8469e+03, -1.0623e+03, -1.2648e+03,\n",
      "         -1.2570e+03, -1.2755e+03, -5.9466e+02, -1.0347e+03,  1.1755e+01],\n",
      "        [-7.4825e+02, -4.9032e+02, -1.4457e+03, -9.9723e+02, -1.0521e+03,\n",
      "         -1.2299e+03, -1.2016e+03, -9.3805e+02, -4.7080e+02,  1.8064e+02],\n",
      "        [-6.0664e+02, -1.9627e+02, -1.3293e+03, -9.3542e+02, -5.0780e+02,\n",
      "         -9.6430e+02, -1.2909e+03, -5.1161e+02, -1.3534e+02,  1.7835e+02],\n",
      "        [-5.1922e+02, -4.7890e+02, -9.6810e+02, -4.7647e+02, -4.6605e+02,\n",
      "         -9.2947e+02, -9.4730e+02, -5.2703e+02, -4.6514e+02, -2.0549e+02],\n",
      "        [-1.6721e+02, -6.5204e+02, -6.8908e+02, -2.2189e+02, -1.0624e+02,\n",
      "         -6.6683e+02, -5.2415e+02, -7.6224e+02, -2.5170e+02,  1.6823e+02],\n",
      "        [-4.9030e+02, -5.2283e+02, -1.1710e+03, -9.5712e+02, -7.7818e+02,\n",
      "         -9.2857e+02, -1.2827e+03, -7.6297e+02, -3.5260e+02, -5.3973e+01],\n",
      "        [-6.1118e+02, -1.9054e+02, -1.1269e+03, -8.8521e+02, -6.9596e+02,\n",
      "         -8.3449e+02, -1.0852e+03, -5.0790e+02, -2.5693e+02, -9.3552e+01],\n",
      "        [-3.5310e+02, -7.6292e+02, -7.1962e+02, -5.9493e+02, -5.4730e+02,\n",
      "         -7.9348e+02, -7.7295e+02, -2.9635e+02, -6.2770e+02, -6.4069e+02],\n",
      "        [-7.3937e+02, -6.6897e+02, -1.4073e+03, -1.0185e+03, -1.1648e+03,\n",
      "         -1.1932e+03, -1.2695e+03, -7.5384e+02, -7.0179e+02, -1.5430e+02],\n",
      "        [-4.4323e+02, -5.0982e+02, -1.1265e+03, -9.7919e+02, -6.6993e+02,\n",
      "         -1.0383e+03, -1.2486e+03, -9.1483e+02, -3.2217e+02,  1.1663e+02],\n",
      "        [-8.1251e+01, -6.4143e+02, -7.6689e+02, -3.2141e+02, -6.4642e+02,\n",
      "         -8.5456e+02, -8.1837e+02, -6.2946e+02, -3.6234e+02,  1.1517e+02],\n",
      "        [-3.6213e+02, -5.0062e+02, -1.0067e+03, -3.2177e+02, -4.9525e+02,\n",
      "         -8.2452e+02, -7.1253e+02, -5.3518e+02, -3.4111e+02,  1.3115e+02],\n",
      "        [-3.5297e+02, -6.6202e+02, -8.8741e+02, -3.4367e+02, -2.6889e+02,\n",
      "         -7.7757e+02, -5.8592e+02, -6.5781e+02, -4.5285e+02, -1.1516e+02],\n",
      "        [-6.1203e+02, -1.2163e+03, -1.2909e+03, -6.2746e+02, -7.0388e+02,\n",
      "         -1.2652e+03, -9.0380e+02, -8.8284e+02, -8.1623e+02, -6.1435e+02],\n",
      "        [-2.0068e+02, -4.6676e+02, -5.7797e+02, -2.4278e+02, -5.0899e+02,\n",
      "         -5.7107e+02, -5.0993e+02, -4.4431e+02, -2.6878e+02, -1.1417e+02],\n",
      "        [-4.6999e+02, -3.8932e+02, -8.0223e+02, -6.4388e+02, -5.2838e+02,\n",
      "         -7.2057e+02, -6.5101e+02, -3.7577e+02, -4.3731e+02, -1.1189e+02],\n",
      "        [-2.7341e+02, -9.0898e+02, -9.3336e+02, -3.9729e+02, -3.6344e+02,\n",
      "         -8.9829e+02, -6.3082e+02, -5.9856e+02, -6.0977e+02, -2.9142e+02],\n",
      "        [-3.6180e+02, -9.9656e+01, -9.4855e+02, -7.3141e+02, -5.4215e+02,\n",
      "         -5.4041e+02, -1.0711e+03, -3.9625e+02, -1.8924e+02, -7.5774e+00],\n",
      "        [-5.9402e+02, -3.6900e+02, -9.0693e+02, -5.8978e+02, -5.5193e+02,\n",
      "         -7.9614e+02, -6.2404e+02, -5.7587e+02, -3.2680e+02,  8.9026e+01],\n",
      "        [-2.8627e+02, -5.4503e+02, -8.4561e+02, -3.4202e+02, -2.8784e+02,\n",
      "         -6.8018e+02, -6.2104e+02, -5.8100e+02, -3.3081e+02,  9.9784e+01],\n",
      "        [-5.5888e+02, -1.7186e+02, -9.3885e+02, -3.1319e+02,  2.4721e+01,\n",
      "         -6.0111e+02, -7.1873e+02, -1.4840e+02, -5.6748e+02,  6.8922e+01],\n",
      "        [-4.8483e+02, -5.2672e+02, -7.1582e+02, -4.7964e+02, -4.9984e+02,\n",
      "         -6.7349e+02, -6.6702e+02, -5.6523e+02, -4.2685e+02, -2.6613e+02],\n",
      "        [-4.6183e+02, -1.4238e+02, -6.3363e+02, -5.0046e+02, -3.7708e+02,\n",
      "         -3.9940e+02, -5.7807e+02, -2.2977e+02, -2.6235e+02, -1.1807e+02],\n",
      "        [-4.0330e+02, -5.3518e+02, -9.5270e+02, -4.0244e+02, -4.1485e+02,\n",
      "         -7.0917e+02, -9.3193e+02, -5.0706e+02, -4.1998e+02, -9.0924e+01],\n",
      "        [-5.1251e+02, -5.2213e+02, -1.3357e+03, -5.3103e+02, -6.4851e+02,\n",
      "         -1.0086e+03, -1.0121e+03, -5.4151e+02, -6.6152e+02,  3.2821e+01],\n",
      "        [-4.1317e+02, -7.1013e+02, -1.2183e+03, -4.0303e+02, -4.5031e+02,\n",
      "         -7.6819e+02, -7.8600e+02, -5.8286e+02, -5.4675e+02,  6.1361e+01],\n",
      "        [-3.5124e+02, -5.1614e+02, -6.9123e+02, -3.8168e+02, -1.7297e+02,\n",
      "         -6.9967e+02, -5.2653e+02, -5.1013e+02, -3.6030e+02, -2.8474e+02],\n",
      "        [-2.4102e+02, -7.5170e+02, -9.0452e+02, -2.5556e+02, -5.0380e+02,\n",
      "         -8.4008e+02, -5.1000e+02, -7.2697e+02, -2.6679e+02,  3.2587e+01],\n",
      "        [-4.4374e+02, -4.9314e+02, -1.1004e+03, -5.2807e+02, -7.4972e+02,\n",
      "         -9.1200e+02, -1.0966e+03, -5.0122e+02, -5.3415e+02, -1.4812e+02],\n",
      "        [-4.6708e+02, -5.8918e+02, -1.5594e+03, -1.0123e+03, -9.7792e+02,\n",
      "         -1.1091e+03, -1.5446e+03, -6.9426e+02, -5.2136e+02, -2.3320e+02],\n",
      "        [-1.9283e+02, -7.9568e+02, -6.3554e+02, -2.4482e+02, -1.4803e+02,\n",
      "         -6.1225e+02, -5.0616e+02, -3.3108e+02, -4.4084e+02, -3.9629e+02],\n",
      "        [-4.5663e+02, -6.5286e+02, -1.1236e+03, -6.8440e+02, -8.8990e+02,\n",
      "         -9.9402e+02, -9.6407e+02, -8.9898e+02, -3.5147e+02,  3.3412e+02],\n",
      "        [-9.4475e+02, -5.8450e+02, -1.6099e+03, -1.0254e+03, -1.0624e+03,\n",
      "         -1.0692e+03, -1.1868e+03, -8.3140e+02, -8.2811e+02, -1.9870e+02],\n",
      "        [-1.9729e+02, -3.3142e+02, -4.2311e+02, -3.1756e+02, -2.8395e+02,\n",
      "         -4.0424e+02, -4.4421e+02, -2.9286e+02, -1.7888e+02, -2.1235e+02],\n",
      "        [-1.8084e+02, -4.3025e+02, -6.0273e+02, -2.9290e+02, -4.7974e+02,\n",
      "         -5.6164e+02, -6.2485e+02, -3.7011e+02, -2.4572e+02, -5.5059e+01],\n",
      "        [-5.4235e+02, -4.0086e+02, -8.2941e+02, -4.5050e+02, -5.7091e+02,\n",
      "         -6.3697e+02, -7.8090e+02, -2.7691e+02, -5.3929e+02, -1.8146e+02],\n",
      "        [-3.1073e+02, -8.5217e+02, -9.6399e+02, -4.0512e+02, -5.8827e+02,\n",
      "         -7.8842e+02, -7.4469e+02, -6.4719e+02, -5.7557e+02, -1.1251e+02],\n",
      "        [-9.5140e+02,  1.5842e+02, -1.6068e+03, -1.1825e+03, -1.0299e+03,\n",
      "         -8.2914e+02, -1.5719e+03, -3.7608e+02, -2.6776e+02,  2.8509e+02],\n",
      "        [-4.3636e+02, -2.5614e+02, -1.2247e+03, -9.7936e+02, -7.1569e+02,\n",
      "         -8.8084e+02, -1.2914e+03, -5.4307e+02, -2.1543e+02,  9.6418e+01],\n",
      "        [-1.9725e+02, -6.9733e+02, -8.4514e+02, -5.3673e+02, -4.4637e+02,\n",
      "         -8.0319e+02, -7.6510e+02, -6.1309e+02, -4.8597e+02, -2.7536e+02],\n",
      "        [-3.7593e+02, -7.6856e+02, -7.9466e+02, -1.8322e+02, -4.0236e+02,\n",
      "         -8.0834e+02, -6.6230e+02, -5.4592e+02, -5.6399e+02, -1.4650e+02],\n",
      "        [-1.5270e+02, -3.8663e+02, -5.7016e+02, -8.9031e+01, -2.4938e+02,\n",
      "         -4.4582e+02, -4.7994e+02, -2.0757e+02, -3.3674e+02, -1.8689e+02],\n",
      "        [-6.0220e+02, -5.4499e+02, -1.1429e+03, -7.3095e+02, -8.5616e+02,\n",
      "         -6.9373e+02, -8.4663e+02, -4.3042e+02, -5.8066e+02, -2.8232e+02],\n",
      "        [-3.8831e+02, -3.3104e+02, -1.1280e+03, -7.9743e+02, -5.6317e+02,\n",
      "         -7.8489e+02, -1.1333e+03, -4.6715e+02, -3.0257e+02, -2.4472e+02],\n",
      "        [-3.4727e+02, -3.1900e+02, -9.8585e+02, -6.2756e+02, -5.4040e+02,\n",
      "         -5.7144e+02, -8.4813e+02, -2.8105e+02, -3.3394e+02, -1.4324e+02],\n",
      "        [-2.9175e+02, -5.1728e+02, -7.0426e+02, -2.5947e+02, -2.8781e+02,\n",
      "         -5.3650e+02, -4.9542e+02, -4.0582e+02, -3.8765e+02, -7.1604e+01],\n",
      "        [-6.1405e+02, -5.4040e+02, -1.4253e+03, -8.1858e+02, -6.8355e+02,\n",
      "         -1.0029e+03, -1.1220e+03, -7.9904e+02, -3.5974e+02,  1.8256e+00],\n",
      "        [-8.8860e+02, -4.2412e+02, -1.3806e+03, -9.6771e+02, -1.1788e+03,\n",
      "         -9.4225e+02, -1.2033e+03, -7.1620e+02, -6.3621e+02, -1.3984e+02],\n",
      "        [-5.2477e+02, -7.8678e+02, -1.0057e+03, -3.7211e+02, -5.7903e+02,\n",
      "         -8.7433e+02, -6.1558e+02, -7.6061e+02, -6.5146e+02, -4.5712e+02],\n",
      "        [-4.2666e+02, -5.4947e+02, -8.7167e+02, -5.8043e+02, -6.0868e+02,\n",
      "         -6.6868e+02, -7.5688e+02, -5.8474e+02, -4.7328e+02, -2.3825e+02],\n",
      "        [-3.3107e+02, -9.0290e+02, -1.1313e+03, -3.4427e+02, -4.0271e+02,\n",
      "         -1.0022e+03, -7.1000e+02, -8.0198e+02, -4.8469e+02,  6.9980e+00],\n",
      "        [-2.1781e+02, -8.5361e+02, -9.1245e+02, -1.9471e+02, -4.2562e+02,\n",
      "         -8.8565e+02, -7.0080e+02, -6.7801e+02, -4.9691e+02, -2.4834e+02],\n",
      "        [-7.8761e+02, -1.8055e+02, -1.4802e+03, -9.2231e+02, -6.9186e+02,\n",
      "         -1.0468e+03, -1.3566e+03, -5.3135e+02, -3.5317e+02,  1.7158e+02],\n",
      "        [-5.5317e+02, -4.5595e+02, -8.3196e+02, -3.8819e+02, -1.6196e+02,\n",
      "         -7.2278e+02, -5.1412e+02, -5.1218e+02, -3.5006e+02, -4.4155e+01],\n",
      "        [-2.9950e+02, -5.4107e+02, -6.9796e+02, -4.1992e+02, -4.0647e+02,\n",
      "         -5.8214e+02, -7.5522e+02, -3.9951e+02, -3.3262e+02, -2.3522e+02],\n",
      "        [-3.5527e+02, -5.8480e+02, -8.3331e+02, -3.7534e+02, -2.7188e+02,\n",
      "         -8.6702e+02, -7.1206e+02, -4.7529e+02, -4.7332e+02, -1.7158e+02],\n",
      "        [-5.3022e+02, -7.5441e+02, -1.0696e+03, -7.4936e+02, -7.4811e+02,\n",
      "         -9.9319e+02, -9.6935e+02, -6.9878e+02, -5.7963e+02, -2.0911e+02],\n",
      "        [-3.8024e+02, -5.0032e+02, -5.8088e+02, -1.2826e+02, -1.7881e+02,\n",
      "         -4.4039e+02, -3.1408e+02, -1.4494e+02, -5.1860e+02, -2.4159e+02],\n",
      "        [-6.4625e+02, -7.0192e+02, -1.1246e+03, -7.1322e+02, -5.7941e+02,\n",
      "         -1.0082e+03, -9.1722e+02, -8.5252e+02, -6.7792e+02, -3.3026e+02],\n",
      "        [-7.5962e+02, -1.0588e+03, -1.1633e+03, -4.1551e+02, -3.0815e+02,\n",
      "         -1.1703e+03, -8.6732e+02, -6.3020e+02, -9.2740e+02, -4.8275e+02],\n",
      "        [-3.5117e+02, -2.8873e+02, -6.0472e+02, -4.7894e+02, -3.9200e+02,\n",
      "         -5.2177e+02, -4.7100e+02, -4.2491e+02, -2.8580e+02, -1.3414e+02],\n",
      "        [-2.0765e+02, -4.8150e+02, -8.0654e+02, -5.0577e+02, -4.3366e+02,\n",
      "         -6.3371e+02, -5.8634e+02, -5.9908e+02, -2.3645e+02,  2.4305e+01],\n",
      "        [-8.9290e+02, -6.2198e+02, -1.2898e+03, -5.5039e+02, -5.6840e+02,\n",
      "         -9.6249e+02, -8.8027e+02, -6.7116e+02, -5.9787e+02,  2.0327e+02],\n",
      "        [-3.1079e+02, -6.4468e+02, -1.0125e+03, -4.9912e+02, -5.2951e+02,\n",
      "         -8.2123e+02, -8.4919e+02, -5.3996e+02, -4.2942e+02, -1.2234e+02],\n",
      "        [-9.2951e+02, -1.9493e+02, -1.7727e+03, -1.1405e+03, -7.0931e+02,\n",
      "         -1.0381e+03, -1.6052e+03, -4.3703e+02, -5.7550e+02, -9.1661e+01],\n",
      "        [-4.8785e+02, -8.6789e+02, -1.0348e+03, -3.9014e+02, -3.0873e+02,\n",
      "         -9.3783e+02, -6.7710e+02, -7.8475e+02, -6.8672e+02, -2.2951e+02],\n",
      "        [-1.2852e+02, -4.8035e+02, -4.2053e+02, -3.1614e+02, -2.8086e+02,\n",
      "         -5.1950e+02, -4.9944e+02, -3.1739e+02, -2.3905e+02, -1.9194e+02],\n",
      "        [-3.6827e+02, -6.8730e+02, -9.2402e+02, -4.7228e+02, -5.9762e+02,\n",
      "         -9.1305e+02, -8.6881e+02, -4.7127e+02, -6.0686e+02, -4.1678e+02],\n",
      "        [-4.9148e+02, -3.6637e+02, -1.3149e+03, -8.2592e+02, -1.0504e+03,\n",
      "         -7.9702e+02, -1.1613e+03, -3.3877e+02, -5.9839e+02, -1.2618e+02],\n",
      "        [-2.9733e+02, -5.4905e+02, -8.2467e+02, -3.0444e+02, -5.5042e+02,\n",
      "         -5.8854e+02, -7.2856e+02, -2.9786e+02, -3.7314e+02, -3.1107e+01],\n",
      "        [-2.0978e+02, -4.7232e+02, -5.1158e+02, -1.7659e+02, -2.5391e+02,\n",
      "         -6.0014e+02, -3.9115e+02, -3.0560e+02, -3.4320e+02, -1.7891e+02],\n",
      "        [-2.5537e+02, -1.3670e+03, -1.2411e+03, -2.9923e+02, -1.6730e+02,\n",
      "         -1.4331e+03, -7.3432e+02, -1.0323e+03, -7.9956e+02, -4.5459e+02],\n",
      "        [-4.3065e+02, -6.2505e+02, -8.5080e+02, -7.0131e+02, -4.9532e+02,\n",
      "         -6.9608e+02, -9.9906e+02, -6.1729e+02, -7.7088e+02, -4.0530e+02],\n",
      "        [-4.2120e+02, -3.3956e+02, -9.7152e+02, -6.5079e+02, -6.0798e+02,\n",
      "         -7.1707e+02, -8.8286e+02, -4.5583e+02, -4.2908e+02, -1.5415e+02],\n",
      "        [-4.0274e+02, -6.5785e+02, -9.4055e+02, -3.6194e+02, -2.8653e+02,\n",
      "         -9.2518e+02, -6.9395e+02, -5.9692e+02, -5.8384e+02, -1.5660e+01],\n",
      "        [-7.3646e+02, -1.0596e+03, -1.6664e+03, -1.0034e+03, -1.0643e+03,\n",
      "         -9.8410e+02, -1.1607e+03, -1.1984e+03, -9.0382e+02, -2.4775e+02],\n",
      "        [-1.0814e+03, -2.9215e+02, -1.4670e+03, -9.6793e+02, -5.0273e+02,\n",
      "         -9.5150e+02, -1.2962e+03, -3.6671e+02, -7.8525e+02,  5.8365e+00],\n",
      "        [-1.9587e+02, -6.3885e+02, -5.6828e+02, -2.5265e+02, -3.8420e+02,\n",
      "         -6.6056e+02, -5.4003e+02, -4.7881e+02, -2.9531e+02, -3.3375e+02],\n",
      "        [-3.1037e+02, -3.0891e+02, -7.7308e+02, -3.7492e+02, -1.8821e+02,\n",
      "         -6.6897e+02, -6.1864e+02, -2.9968e+02, -3.2959e+02,  5.8617e+01],\n",
      "        [-8.0648e+02, -1.1600e+03, -1.2020e+03, -5.0520e+02, -5.3349e+02,\n",
      "         -1.3874e+03, -7.3781e+02, -8.9800e+02, -9.3468e+02, -4.7809e+02],\n",
      "        [-2.5782e+02, -9.4948e+02, -1.0414e+03, -7.4897e+02, -6.7489e+02,\n",
      "         -1.1189e+03, -9.7994e+02, -8.0426e+02, -4.2002e+02, -3.6858e+02],\n",
      "        [-5.0643e+02, -5.5010e+02, -1.0940e+03, -7.5175e+02, -6.8850e+02,\n",
      "         -9.1678e+02, -9.5233e+02, -8.4610e+02, -4.7027e+02,  4.4001e+01],\n",
      "        [-6.2203e+02, -4.5624e+02, -1.3968e+03, -9.6981e+02, -6.3477e+02,\n",
      "         -1.0102e+03, -1.1831e+03, -4.4160e+02, -8.5602e+02, -5.1094e+02],\n",
      "        [-5.1237e+02, -2.8932e+02, -1.0973e+03, -6.1654e+02, -4.7645e+02,\n",
      "         -7.6885e+02, -8.3967e+02, -4.8246e+02, -5.0773e+02, -1.2705e+02],\n",
      "        [-8.2273e+02, -7.2434e+02, -1.6774e+03, -9.6563e+02, -1.2480e+03,\n",
      "         -1.2985e+03, -1.7601e+03, -7.3421e+02, -8.1147e+02, -3.7646e+02],\n",
      "        [-1.4747e+02, -4.1940e+02, -7.1964e+02, -4.9795e+02, -3.4786e+02,\n",
      "         -6.5935e+02, -7.9331e+02, -3.9097e+02, -2.8261e+02, -8.9014e+01],\n",
      "        [-2.3856e+02, -1.9017e+02, -7.5236e+02, -5.8659e+02, -5.1492e+02,\n",
      "         -5.5922e+02, -8.2459e+02, -3.5860e+02, -1.6720e+02,  3.6315e+01],\n",
      "        [-7.4263e+02, -5.8277e+02, -1.4033e+03, -1.0781e+03, -9.6461e+02,\n",
      "         -1.0830e+03, -1.1959e+03, -6.1158e+02, -7.3654e+02, -3.2749e+02],\n",
      "        [-2.1418e+02, -4.3552e+02, -9.4013e+02, -2.8610e+02, -4.7676e+02,\n",
      "         -7.9045e+02, -8.3956e+02, -4.6684e+02, -2.2681e+02,  6.2590e+01],\n",
      "        [-6.9922e+02, -1.4969e+02, -1.5628e+03, -1.0233e+03, -7.2462e+02,\n",
      "         -8.8750e+02, -1.5707e+03, -4.7067e+02, -5.1856e+02,  7.0375e+01],\n",
      "        [-1.0889e+03, -6.8425e+02, -2.0962e+03, -1.3508e+03, -1.4659e+03,\n",
      "         -1.4757e+03, -2.0146e+03, -8.3893e+02, -8.1359e+02, -5.6409e+01],\n",
      "        [-4.8631e+02, -7.1664e+02, -1.1439e+03, -5.4026e+02, -3.6345e+02,\n",
      "         -1.0091e+03, -8.8607e+02, -9.5779e+02, -5.1703e+02, -1.8914e+02]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "student_input = next(iter(preprocessed_dataloaders['test']))  \n",
    "start = time.time()\n",
    "student_output = student(student_input['img'])\n",
    "print(\"student: \", time.time()-start)\n",
    "print(student_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_calculator = LossCalulcator(temperature, distillw)\n",
    "optimizer = optim.AdamW(\n",
    "    student.parameters(),\n",
    "    lr=lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer = optimizer,\n",
    "                                    step_size = lr_stepsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [07:14<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(student \u001b[38;5;241m=\u001b[39m student,\n\u001b[1;32m      2\u001b[0m     teacher \u001b[38;5;241m=\u001b[39m teacher,\n\u001b[1;32m      3\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m preprocessed_dataloaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplitted_train\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      4\u001b[0m     val_dataloader \u001b[38;5;241m=\u001b[39m preprocessed_dataloaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      5\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m optimizer,\n\u001b[1;32m      6\u001b[0m     scheduler \u001b[38;5;241m=\u001b[39m scheduler,\n\u001b[1;32m      7\u001b[0m     loss_calculator \u001b[38;5;241m=\u001b[39m loss_calculator,\n\u001b[1;32m      8\u001b[0m     epochs \u001b[38;5;241m=\u001b[39m epochs,\n\u001b[1;32m      9\u001b[0m     device \u001b[38;5;241m=\u001b[39m device,\n\u001b[1;32m     10\u001b[0m     )\n",
      "File \u001b[0;32m~/cornell/ml_systems/cs6787_tsm/pretrained_kd.py:21\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(student, teacher, dataloader, val_dataloader, optimizer, scheduler, loss_calculator, epochs, device)\u001b[0m\n\u001b[1;32m     17\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# train one epoch\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     train_step(student, teacher, dataloader, optimizer, loss_calculator, epoch, device)\n\u001b[1;32m     23\u001b[0m     duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# validate the network\u001b[39;00m\n",
      "File \u001b[0;32m~/cornell/ml_systems/cs6787_tsm/pretrained_kd.py:60\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(student, teacher, dataloader, optimizer, loss_calculator, epoch, device)\u001b[0m\n\u001b[1;32m     57\u001b[0m outputs \u001b[38;5;241m=\u001b[39m student(info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 60\u001b[0m     teacher_outputs \u001b[38;5;241m=\u001b[39m teacher(info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     62\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_calculator(outputs          \u001b[38;5;241m=\u001b[39m outputs,\n\u001b[1;32m     63\u001b[0m                        labels           \u001b[38;5;241m=\u001b[39m info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     64\u001b[0m                        teacher_outputs  \u001b[38;5;241m=\u001b[39m teacher_outputs)\n\u001b[1;32m     65\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/transformers/models/vit/modeling_vit.py:791\u001b[0m, in \u001b[0;36mViTForImageClassification.forward\u001b[0;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;124;03m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    789\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 791\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvit(\n\u001b[1;32m    792\u001b[0m     pixel_values,\n\u001b[1;32m    793\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m    794\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    795\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    796\u001b[0m     interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding,\n\u001b[1;32m    797\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    798\u001b[0m )\n\u001b[1;32m    800\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    802\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output[:, \u001b[38;5;241m0\u001b[39m, :])\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/transformers/models/vit/modeling_vit.py:574\u001b[0m, in \u001b[0;36mViTModel.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    568\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mto(expected_dtype)\n\u001b[1;32m    570\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    571\u001b[0m     pixel_values, bool_masked_pos\u001b[38;5;241m=\u001b[39mbool_masked_pos, interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding\n\u001b[1;32m    572\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    575\u001b[0m     embedding_output,\n\u001b[1;32m    576\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m    577\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    578\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    579\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    580\u001b[0m )\n\u001b[1;32m    581\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    582\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(sequence_output)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/transformers/models/vit/modeling_vit.py:404\u001b[0m, in \u001b[0;36mViTEncoder.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    397\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    398\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    399\u001b[0m         hidden_states,\n\u001b[1;32m    400\u001b[0m         layer_head_mask,\n\u001b[1;32m    401\u001b[0m         output_attentions,\n\u001b[1;32m    402\u001b[0m     )\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 404\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(hidden_states, layer_head_mask, output_attentions)\n\u001b[1;32m    406\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/transformers/models/vit/modeling_vit.py:349\u001b[0m, in \u001b[0;36mViTLayer.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    346\u001b[0m     head_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    347\u001b[0m     output_attentions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    348\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[0;32m--> 349\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[1;32m    350\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm_before(hidden_states),  \u001b[38;5;66;03m# in ViT, layernorm is applied before self-attention\u001b[39;00m\n\u001b[1;32m    351\u001b[0m         head_mask,\n\u001b[1;32m    352\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    353\u001b[0m     )\n\u001b[1;32m    354\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    355\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/transformers/models/vit/modeling_vit.py:291\u001b[0m, in \u001b[0;36mViTAttention.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    287\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    288\u001b[0m     head_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    289\u001b[0m     output_attentions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    290\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[0;32m--> 291\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(hidden_states, head_mask, output_attentions)\n\u001b[1;32m    293\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    295\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/transformers/models/vit/modeling_vit.py:209\u001b[0m, in \u001b[0;36mViTSelfAttention.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m, hidden_states, head_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, output_attentions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    208\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[0;32m--> 209\u001b[0m     mixed_query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery(hidden_states)\n\u001b[1;32m    211\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(hidden_states))\n\u001b[1;32m    212\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(hidden_states))\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(student = student,\n",
    "    teacher = teacher,\n",
    "    dataloader = preprocessed_dataloaders['splitted_train'],\n",
    "    val_dataloader = preprocessed_dataloaders['validation'],\n",
    "    optimizer = optimizer,\n",
    "    scheduler = scheduler,\n",
    "    loss_calculator = loss_calculator,\n",
    "    epochs = epochs,\n",
    "    device = device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_accuracy(student, preprocessed_dataloaders['test'], device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
